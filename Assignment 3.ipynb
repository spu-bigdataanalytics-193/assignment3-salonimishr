{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries for Map Reduce Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import data_handler\n",
    "import util\n",
    "import functools\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Downloaded the data from \"http://stat-computing.org/dataexpo/2009/\" website. Using user-defined library data_handler. The dowload status shows the task completed 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download status:  [####################] 100% \n"
     ]
    }
   ],
   "source": [
    "data_handler.download_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating & Sorting data files\n",
    "\n",
    "Funtion glob.glob is used to list the data files with their full path and then got sorted by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all files under data folder\n",
    "file_list = sorted(glob.glob(os.path.join('data', '*.csv.bz2')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data as dataframe\n",
    "\n",
    "The whole data which is stored in 22 data files in zip format(.bz2) consists of the size of 12gb. The system random access memory was limited and do not allow to load the complete data. As suggested, only the data important for map reduce is loaded to surpass the difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_names=['Year', 'UniqueCarrier']\n",
    "df1 = pd.DataFrame()\n",
    "for ind, data_file in enumerate(file_list):\n",
    "    # read current data\n",
    "    df = pd.DataFrame(pd.read_csv(data_file, encoding='ISO-8859-1', memory_map=True, low_memory=False, usecols=['Year', 'UniqueCarrier']))\n",
    "    df1=df1.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking the shape of loaded data\n",
    "\n",
    "The len() shows data consists of 123 million of records from year 1987 to 2008. Using the head and tail function reflects that the complete  data has been loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Year UniqueCarrier\n",
      "7009723  2008            DL\n",
      "7009724  2008            DL\n",
      "7009725  2008            DL\n",
      "7009726  2008            DL\n",
      "7009727  2008            DL\n",
      "   Year UniqueCarrier\n",
      "0  1987            PS\n",
      "1  1987            PS\n",
      "2  1987            PS\n",
      "3  1987            PS\n",
      "4  1987            PS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123534969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df1.tail())\n",
    "print(df1.head())\n",
    "len(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the job without map reduce\n",
    "\n",
    "The Unique Carrier and their frequency is obtained without using map reduce algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (123534969, 2) ; 112.16 Mb [####################] 100% \n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "carrier_counts = {}\n",
    "# unique airlines in dataset\n",
    "carriers = df1.UniqueCarrier.unique()\n",
    "    \n",
    "    # update the global carrier_count\n",
    "for key in carriers:\n",
    "    if key not in carrier_counts:\n",
    "        carrier_counts.update({key: 0})\n",
    "    \n",
    "# loop through each row in dataframe \n",
    "for carrier in df1.UniqueCarrier:\n",
    "    carrier_counts[carrier] += 1\n",
    "\n",
    "# info \n",
    "prefix ='Shape: {} ; {} Mb'.format(\n",
    "        df1.shape, round(df.memory_usage().sum() / 1e+6,2))\n",
    "    \n",
    "# track the progress\n",
    "util.progressbar(len(file_list), ind + 1, prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PS': 83617,\n",
       " 'TW': 3757747,\n",
       " 'UA': 13299817,\n",
       " 'WN': 15976022,\n",
       " 'EA': 919785,\n",
       " 'HP': 3636682,\n",
       " 'NW': 10292627,\n",
       " 'PA (1)': 316167,\n",
       " 'PI': 873957,\n",
       " 'CO': 8145788,\n",
       " 'DL': 16547870,\n",
       " 'AA': 14984647,\n",
       " 'US': 14075530,\n",
       " 'AS': 2878021,\n",
       " 'ML (1)': 70622,\n",
       " 'AQ': 154381,\n",
       " 'MQ': 3954895,\n",
       " 'OO': 3090853,\n",
       " 'XE': 2350309,\n",
       " 'TZ': 208420,\n",
       " 'EV': 1697172,\n",
       " 'FL': 1265138,\n",
       " 'B6': 811341,\n",
       " 'DH': 693047,\n",
       " 'HA': 274265,\n",
       " 'OH': 1464176,\n",
       " 'F9': 336958,\n",
       " 'YV': 854056,\n",
       " '9E': 521059}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "carrier_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv(\"result.csv\") ## Saving the data in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce way\n",
    "\n",
    "Map reduce way - map, reduce and sort, and collect way to count the flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123534969"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carriers = df1.UniqueCarrier\n",
    "len(carriers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Phase\n",
    "\n",
    "For each line of flight data, I extracted the carrier code and make a key value pair as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mapping=map((lambda x: (x, 1)), carriers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueValues = set(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TW', 1)\n",
      "('HA', 1)\n",
      "('UA', 1)\n",
      "('PA (1)', 1)\n",
      "('TZ', 1)\n",
      "('FL', 1)\n",
      "('ML (1)', 1)\n",
      "('DL', 1)\n",
      "('AS', 1)\n",
      "('WN', 1)\n",
      "('PI', 1)\n",
      "('EV', 1)\n",
      "('US', 1)\n",
      "('YV', 1)\n",
      "('AA', 1)\n",
      "('XE', 1)\n",
      "('F9', 1)\n",
      "('OH', 1)\n",
      "('MQ', 1)\n",
      "('NW', 1)\n",
      "('AQ', 1)\n",
      "('EA', 1)\n",
      "('PS', 1)\n",
      "('CO', 1)\n",
      "('9E', 1)\n",
      "('B6', 1)\n",
      "('OO', 1)\n",
      "('DH', 1)\n",
      "('HP', 1)\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "for value in uniqueValues:  \n",
    "    print(value)  \n",
    "print(len(uniqueValues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle and Sort Phase\n",
    "\n",
    "In this phase, I listed the pairs from the map phase and grouped all values of each key together. After this, I sorted the data by the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sorted_mapping=sorted(mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Phase\n",
    "\n",
    "In this phase, I read each key and list of values from shuffle and sort phase. I also added the total # of ones in the carrier code's list together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('9E', 521059), ('AA', 14984647), ('AQ', 154381), ('AS', 2878021), ('B6', 811341), ('CO', 8145788), ('DH', 693047), ('DL', 16547870), ('EA', 919785), ('EV', 1697172), ('F9', 336958), ('FL', 1265138), ('HA', 274265), ('HP', 3636682), ('ML (1)', 70622), ('MQ', 3954895), ('NW', 10292627), ('OH', 1464176), ('OO', 3090853), ('PA (1)', 316167), ('PI', 873957), ('PS', 83617), ('TW', 3757747), ('TZ', 208420), ('UA', 13299817), ('US', 14075530), ('WN', 15976022), ('XE', 2350309), ('YV', 854056)]\n",
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "grouper=groupby(sorted_mapping, lambda p:p[0])\n",
    "final=map(lambda l: (l[0], reduce(lambda x, y:x+y,\n",
    "                                 map(lambda p: p[1], l[1]))), grouper)\n",
    "\n",
    "print(list(final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Map reduce is very functional algorithm where three parts of it can easily executed. In this assignment, I tried to implement map reduce algorithm into a >100 million rows of dataset. I have a dataset of size 1.6 GB (compressed, and 12 GB when uncompressed), with the records of nearly 120 million records. The goal is to use map-reduce algorithm to get the count of number of flights for each carrier.\n",
    "\n",
    "For comparison, I used two ways of getting counts of carriers.\n",
    "\n",
    "Looping through each record and counting each airline's flight.\n",
    "\n",
    "Map reduce way - map, reduce and sort, and collect way to count the flights.\n",
    "\n",
    "Hence it can be concluded from above, map-reduce does not only take less time but also easy to implement.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
